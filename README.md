# Uncertainty Adversarial Robustness
[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/drive/1vVbEmkDVWsgPJMSMEnc_aBqPn6P7YrkG?usp=sharing)
[![Arxiv](https://github.com/EmanueleLedda97/UncertaintyAdversarialRobustness/blob/code_refactoring/arxive_button.svg)](https://arxiv.org/)

This repository contains all the code used for running the experiments conducted on our work **On the Robustness of Adversarially Trained Models against Uncertainty Attack**, submitted at Pattern Recognition, August 2024.

![graphical abstract](https://github.com/EmanueleLedda97/UncertaintyAdversarialRobustness/blob/code_refactoring/graphical_abstract.pdf)

## Quick Tests :test_tube:
From the colab it is possible to see the over- and under-confidence attacks in action with a quick snippet of code, visualizing the uncertainty span of any sample on any RobustBench model.

## Reproducing the Experiments :microscope:
The file `main_attack.py` can be used for running a single experiment on a RobustBench model. It takes a list of arguments: TBC
